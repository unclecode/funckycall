from pydantic import BaseModel, Field
from typing import Optional, Dict
from .base import ReasoningBase
from pydantic import Field
from typing import Optional
import requests
import json, os
from providers import GroqProvider
import concurrent.futures
from dotenv import load_dotenv
load_dotenv()

def get_rerank_prompt(query, responses, top_k):
    prompt = f"""You are an AI assistant tasked with evaluating and selecting the best responses to a user's request. The user's request is:

<context>
{query}
</context>

Here are the responses generated by different programmers to the user's request:

{responses}

# Task:
Your task is to evaluate these responses and select the top {top_k} that best address the user's request. Consider factors such as relevance, clarity, and completeness when making your selection.

After selecting the top {top_k} responses, generate a final response by merging and summarizing the selected responses. Format your output as follows:

**Make sure to wrap the final answer supposed to be back to user using >>>**

# Example of your response:
After evaluating the responses, I have selected the top {top_k} that best address the user's request. Here they are:

<response index = 1>
summary of response 1

<response index = 2>
summary of response 2

...

<response index = top_k>
summary of response top_k

Based on these top {top_k} responses, I have generated a final response by merging and summarizing them:

>>>
final_response_for_user
>>>"""

    return prompt

class RerankReasoning(ReasoningBase):
    name = "rerank"
    description = "Use this reasoning strategy to generate and rerank responses to a user query."

    def __init__(self, generator_model: str, reranker_model: str, n: int = 5, top_k: int = 3):
        self.generator_model = generator_model
        self.reranker_model = reranker_model
        self.n = n
        self.top_k = top_k
        self.generator_provider = GroqProvider(api_key=os.getenv("GROQ_API_KEY"))
        self.reranker_provider = GroqProvider(api_key=os.getenv("GROQ_API_KEY"))

    def run(self, context):
        message_stories = context.messages

        # Generate responses in parallel using a thread pool
        with concurrent.futures.ThreadPoolExecutor() as executor:
            response_futures = [executor.submit(self.generator_provider.route, model=self.generator_model, messages=message_stories) for _ in range(self.n)]
            responses = [future.result().get("response") for future in concurrent.futures.as_completed(response_futures)]

        unique_responses = list(set(responses))

        # Adjust top_k if it's greater than the number of unique responses
        if self.top_k > len(unique_responses):
            self.top_k = int(len(unique_responses) * 0.4)

        # Generate prompt for reranking
        prompt = get_rerank_prompt(
            query=message_stories[-1].content,
            responses='\n\n'.join([f'<response index={idx + 1}>\n{response}\n</response>' for idx, response in enumerate(unique_responses)]),
            top_k=self.top_k
        )

        # Rerank responses
        rerank_completion = self.reranker_provider.route(model=self.reranker_model, messages=[{"content": prompt}])
        reranked_response = rerank_completion.get("response")

        # Extract the final response
        final_response = reranked_response.split(">>>")[1].split(">>>")[0].strip()

        # Add the final response as a new message to the context
        new_message = {"role": "assistant", "content":final_response}
        context.messages.append(new_message)

        return new_message